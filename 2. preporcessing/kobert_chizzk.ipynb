{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 필요 모듈 설치"
      ],
      "metadata": {
        "id": "xrs7Q9CmcqDI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taJHS3bM9Fct",
        "outputId": "2d9ce794-682b-405b-807f-c364dc630707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.2.2)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.25.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-24mayy1j/kobert-tokenizer_eb3428439d03413f8480f2bb6347fe00\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-24mayy1j/kobert-tokenizer_eb3428439d03413f8480f2bb6347fe00\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 필요 라이브러리 임포트"
      ],
      "metadata": {
        "id": "b_q6A_2FczP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.bool = np.bool_\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "hAfaBnxF9NbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 데이터 불러오기 및 feature 선택"
      ],
      "metadata": {
        "id": "uL-f2vd1dRfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chzzk_review = pd.read_csv(\"/content/drive/MyDrive/치지직리뷰.csv\")"
      ],
      "metadata": {
        "id": "hFmjZrPRdMew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = chzzk_review[\"content\"]"
      ],
      "metadata": {
        "id": "k9fO_MxudOPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_list = reviews.tolist()"
      ],
      "metadata": {
        "id": "IP_iju_mdPw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(review_list)"
      ],
      "metadata": {
        "id": "f729lCo3dQ9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 모델 불러오기"
      ],
      "metadata": {
        "id": "jEZu-tUJc4Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kobert pt 파일이 들어있는 경로\n",
        "path = \"/content/drive/MyDrive/kobertfinetuning_epoch_3.pt\""
      ],
      "metadata": {
        "id": "TGdLXj0M9fVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZTGDdB49gqX",
        "outputId": "1219af8a-eee2-48f5-dea7-4b9be9d27202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=3,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "uecyEKII9oRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tok = tokenizer.tokenize\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "kc6YNAsT9pWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kobert 모델 호출\n",
        "states = torch.load(path)"
      ],
      "metadata": {
        "id": "vm9PIzs99s7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert Classifier 객체 설정\n",
        "model = BERTClassifier(bert=bertmodel, dr_rate=0.5,)\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8jOgZbq9vS2",
        "outputId": "5042b90a-3db3-4aec-a5d6-0f4fbb17e4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter 지정\n",
        "max_len = 64\n",
        "batch_size = 1990\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "qtDIMeTo-Ion"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_softmax(a) :\n",
        "    c = np.max(a) # 최댓값\n",
        "    exp_a = np.exp(a-c) # 각각의 원소에 최댓값을 뺀 값에 exp를 취한다. (이를 통해 overflow 방지)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = (exp_a / sum_exp_a) * 100\n",
        "    return np.round(y, 3)"
      ],
      "metadata": {
        "id": "8JBSV4f6-JuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 모델 사용 & output을 csv파일로 변경"
      ],
      "metadata": {
        "id": "gGXctvucdlZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def predict(predict_sentences):\n",
        "    dataset_another = [(sentence, '0') for sentence in predict_sentences]\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.to(device).long()\n",
        "        segment_ids = segment_ids.to(device).long()\n",
        "        valid_length = valid_length.to(device)\n",
        "        label = label.to(device).long()\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "        for batch_index, logits in enumerate(out):\n",
        "            if torch.argmax(logits) == 2:  # 긍정인 경우에만 추가\n",
        "               predictions.append((predict_sentences[batch_id * batch_size + batch_index], \"긍정\"))\n",
        "            elif torch.argmax(logits) == 0:  # 부정인 경우에만 추가\n",
        "               predictions.append((predict_sentences[batch_id * batch_size + batch_index], \"부정\"))\n",
        "            else : # 중립인 경우에 추가\n",
        "               predictions.append((predict_sentences[batch_id * batch_size + batch_index], \"중립\"))\n",
        "\n",
        "    return pd.DataFrame(predictions)\n"
      ],
      "metadata": {
        "id": "bf17xSzTtil3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict200 = predict(review_list[:200])\n",
        "predict400 = predict(review_list[200:400])\n",
        "predict600 = predict(review_list[400:600])\n",
        "predict800 = predict(review_list[600:800])\n",
        "predict1000 = predict(review_list[800:1000])\n",
        "predict1200 = predict(review_list[1000:1200])\n",
        "predict1400 = predict(review_list[1200:1400])\n",
        "predict1600 = predict(review_list[1400:1600])\n",
        "predict1800 = predict(review_list[1600:1800])\n",
        "predict1990 = predict(review_list[1800:])"
      ],
      "metadata": {
        "id": "kDrWZNdDBi3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chzzk_all= pd.concat([predict200,predict400,predict600, predict800, predict1000, predict1200, predict1400, predict1600, predict1800, predict1990 ],axis=0)"
      ],
      "metadata": {
        "id": "DMO94gJJzO11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 긍정만 모아놓는 csv와 전체를 다 모아놓는 csv 2개를 만들었다.\n",
        "chzzk_all = chzzk_all.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "LPZykXM9zO4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chzzk_positive = pd.DataFrame(chzzk_positive)"
      ],
      "metadata": {
        "id": "cX09rcxEzO6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chzzk_positive.columns = ['content']"
      ],
      "metadata": {
        "id": "9eNgWsnSzO8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chzzk_positive.to_csv('chzzk_positive.csv', index=False)"
      ],
      "metadata": {
        "id": "fimNZSMTzO_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chzzk_all.columns = ['content', 'emotion']"
      ],
      "metadata": {
        "id": "kScz7hndzPBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chzzk_all.to_csv('chzzk_all.csv', index=False)"
      ],
      "metadata": {
        "id": "BlWoyNb-zPDS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}